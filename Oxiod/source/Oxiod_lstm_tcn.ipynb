{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7927fbb-79ca-4fb3-a38c-b11ba41a72bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://192.168.16.2:8080/root/pypi/+simple/, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.22.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (23.0)\n",
      "Requirement already satisfied: protobuf>=4.22.3 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (4.23.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: http://192.168.16.2:8080/root/pypi/+simple/, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy-quaternion in /usr/local/lib/python3.8/dist-packages (2022.4.3)\n",
      "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.8/dist-packages (from numpy-quaternion) (1.22.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "! pip install tensorboardX\n",
    "! pip install numpy-quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df3e435-b649-4f4f-981c-bf7883f5038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path as osp\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from model_temporal import LSTMSeqNetwork, BilinearLSTMSeqNetwork, TCNSeqNetwork\n",
    "from utils import load_config, MSEAverageMeter\n",
    "from data_Oxiod import *\n",
    "from transformations import ComposeTransform, RandomHoriRotateSeq\n",
    "from metric import compute_absolute_trajectory_error, compute_relative_trajectory_error\n",
    "\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "# _nano_to_sec = 1e09\n",
    "# device = 'cpu'\n",
    "global _input_channel, _output_channel\n",
    "_input_channel, _output_channel = 6, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b7baa8-0edb-4275-b502-be281ea79aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace()\n",
    "\n",
    "#Paths to different files\n",
    "args.type = 'lstm' #choices=['tcn', 'lstm', 'lstm_bi']\n",
    "args.root_dir = '/home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/handheld'\n",
    "args.train_list = osp.join(args.root_dir, 'Train-small.txt')\n",
    "args.val_list = osp.join(args.root_dir, 'Validation.txt')\n",
    "args.test_list = osp.join(args.root_dir, 'Test.txt')\n",
    "args.out_dir = osp.join(args.root_dir, args.type + '_outputs')\n",
    "args.model_path = osp.join(args.out_dir, 'checkpoints/checkpoint_latest.pt')\n",
    "\n",
    "\n",
    "\n",
    "args.test_path = None\n",
    "args.cache_path = None\n",
    "args.continue_from = None\n",
    "args.transfer_from = None\n",
    "\n",
    "\n",
    "# Params to tune for neural network\n",
    "args.mode = \"train\"\n",
    "args.step_size = 10\n",
    "args.window_size = 400\n",
    "args.lr = 1e-04\n",
    "args.batch_size = 256\n",
    "args.epochs = 80\n",
    "args.freeze_params = False\n",
    "args.save_interval = 20\n",
    "\n",
    "# # tcn\n",
    "args.kernel_size = 3\n",
    "args.channels = [32,64,128,256,72,36]\n",
    "# # lstm\n",
    "args.layers = 3\n",
    "args.layer_size = 100\n",
    "\n",
    "args.cpu = False\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
    "args.run_ekf = False\n",
    "args.fast_test = False\n",
    "\n",
    "# Plots and animation\n",
    "args.show_plot = True\n",
    "args.saveAnim = False\n",
    "\n",
    "# Smoothing feature and targets\n",
    "args.feature_sigma = 2\n",
    "args.target_sigma = 2\n",
    "\n",
    "np.set_printoptions(formatter={'all': lambda x: '{:.6f}'.format(x)})\n",
    "\n",
    "def get_model(args, **kwargs):\n",
    "    config = {}\n",
    "    if kwargs.get('dropout'):\n",
    "        config['dropout'] = kwargs.get('dropout')\n",
    "\n",
    "    if args.type == 'tcn':\n",
    "        network = TCNSeqNetwork(_input_channel, _output_channel, args.kernel_size,\n",
    "                                layer_channels=args.channels, **config)\n",
    "        print(\"TCN Network. Receptive field: {} \".format(network.get_receptive_field()))\n",
    "    elif args.type == 'lstm_bi':\n",
    "        print(\"Bilinear LSTM Network\")\n",
    "        network = BilinearLSTMSeqNetwork(_input_channel, _output_channel, args.batch_size, device,\n",
    "                                         lstm_layers=args.layers, lstm_size=args.layer_size, **config).to(device)\n",
    "    else:\n",
    "        print(\"Simple LSTM Network\")\n",
    "        network = LSTMSeqNetwork(_input_channel, _output_channel, args.batch_size, device,\n",
    "                                 lstm_layers=args.layers, lstm_size=args.layer_size, **config).to(device)\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "    print('Network constructed. trainable parameters: {}'.format(pytorch_total_params))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd7eec5-4c0c-4080-821d-e8b2e2725a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalPosLoss(torch.nn.Module):\n",
    "    def __init__(self, mode='full', history=None):\n",
    "        \"\"\"\n",
    "        Calculate position loss in global coordinate frame\n",
    "        Target :- Global Velocity\n",
    "        Prediction :- Global Velocity\n",
    "        \"\"\"\n",
    "        super(GlobalPosLoss, self).__init__()\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "        assert mode in ['full', 'part']\n",
    "        self.mode = mode\n",
    "        if self.mode == 'part':\n",
    "            assert history is not None\n",
    "            self.history = history\n",
    "        elif self.mode == 'full':\n",
    "            self.history = 1\n",
    "\n",
    "    # shape = [num_dimensions, sequence_length]\n",
    "    def forward(self, pred, targ):\n",
    "        gt_pos = torch.cumsum(targ[:, 1:, ], 1)\n",
    "        pred_pos = torch.cumsum(pred[:, 1:, ], 1)\n",
    "        if self.mode == 'part':\n",
    "            gt_pos = gt_pos[:, self.history:, :] - gt_pos[:, :-self.history, :]\n",
    "            pred_pos = pred_pos[:, self.history:, :] - pred_pos[:, :-self.history, :]\n",
    "        loss = self.mse_loss(pred_pos, gt_pos)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e7158e-ad98-4253-b636-dc289e12982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_config(args, **kwargs):\n",
    "    if args.out_dir:\n",
    "        with open(osp.join(args.out_dir, 'config.json'), 'w') as f:\n",
    "            values = vars(args)\n",
    "            values['file'] = \"pytorch_global_position\"\n",
    "            if kwargs:\n",
    "                values['kwargs'] = kwargs\n",
    "            json.dump(values, f, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67343bf1-9359-4d1b-b9c6-ef70f2ec7d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(root_dir, data_list, args, **kwargs):\n",
    "    global _output_channel\n",
    "    input_format, output_format = [0, 3, 6], [0, _output_channel]\n",
    "    mode = kwargs.get('mode', 'train')\n",
    "\n",
    "    random_shift, shuffle, transforms, grv_only = 0, False, [], False\n",
    "    if mode == 'train':\n",
    "        args.feature_sigma = 2\n",
    "        args.target_sigma = 2\n",
    "        random_shift = args.step_size // 2\n",
    "        shuffle = True\n",
    "        transforms.append(RandomHoriRotateSeq(input_format, output_format))\n",
    "    elif mode == 'val':\n",
    "        shuffle = True\n",
    "        args.feature_sigma = 1\n",
    "        args.target_sigma = -1\n",
    "    elif mode == 'test':\n",
    "        shuffle = False\n",
    "        grv_only = True\n",
    "        args.feature_sigma = 2\n",
    "        args.target_sigma = -1\n",
    "    transforms = ComposeTransform(transforms)\n",
    "    \n",
    "    seq_type = OxfordGlobSpeedSequence\n",
    "    print('Parameters into dataset initialisation \\n feat_sigma: {}, targ_sigma: {}, random_shift: {}, transform: {}'.format(args.feature_sigma, args.target_sigma, random_shift, transforms))\n",
    "    dataset = SequenceToSequenceDataset(seq_type, root_dir, data_list, args.cache_path, args.step_size, args.window_size, shuffle = shuffle, \n",
    "                                     grv_only=grv_only, transform = None, random_shift = random_shift, feature_sigma = args.feature_sigma, target_sigma = args.target_sigma)\n",
    "    print(f'step_size: {dataset.step_size}, window_size: {dataset.window_size}')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_dataset_from_list(root_dir, list_path, args, **kwargs):\n",
    "    with open(list_path) as f:\n",
    "        data_list = [s.strip().split(',' or ' ')[0] for s in f.readlines() if len(s) > 0 and s[0] != '#']\n",
    "    return get_dataset(root_dir, data_list, args, **kwargs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d7e1347-7b35-47d5-8e65-c2b695554e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(history, args, **kwargs):\n",
    "    if args.type == 'tcn':\n",
    "        config = {'mode': 'part',\n",
    "                  'history': history}\n",
    "    else:\n",
    "        config = {'mode': 'full'}\n",
    "\n",
    "    criterion = GlobalPosLoss(**config)\n",
    "    return criterion\n",
    "\n",
    "\n",
    "def format_string(*argv, sep=' '):\n",
    "    result = ''\n",
    "    for val in argv:\n",
    "        if isinstance(val, (tuple, list, np.ndarray)):\n",
    "            for v in val:\n",
    "                result += format_string(v, sep=sep) + sep\n",
    "        else:\n",
    "            result += str(val) + sep\n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91cf768f-f37f-4207-8051-fb0a6a46f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, **kwargs):\n",
    "    # Loading data\n",
    "    start_t = time.time()\n",
    "    train_dataset = get_dataset_from_list(args.root_dir, args.train_list, args, mode='train', **kwargs)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    end_t = time.time()\n",
    "\n",
    "    print('Training set loaded. Time usage: {:.3f}s'.format(end_t - start_t))\n",
    "    val_dataset, val_loader = None, None\n",
    "    if args.val_list is not None:\n",
    "        val_dataset = get_dataset_from_list(args.root_dir, args.val_list, args, mode='val', **kwargs)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "        print('Validation set loaded')\n",
    "\n",
    "    global device\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "    if args.out_dir is not None:\n",
    "        if not osp.isdir(args.out_dir):\n",
    "            os.makedirs(args.out_dir)\n",
    "        #write_config(args)\n",
    "            \n",
    "        if not osp.isdir(osp.join(args.out_dir, 'checkpoints')):\n",
    "            os.makedirs(osp.join(args.out_dir, 'checkpoints'))\n",
    "        if not osp.isdir(osp.join(args.out_dir, 'logs')):\n",
    "            os.makedirs(osp.join(args.out_dir, 'logs'))\n",
    "\n",
    "    print('\\nNumber of train samples: {}'.format(len(train_dataset)))\n",
    "    train_mini_batches = len(train_loader)\n",
    "    if val_dataset:\n",
    "        print('Number of val samples: {}'.format(len(val_dataset)))\n",
    "        val_mini_batches = len(val_loader)\n",
    "\n",
    "    network = get_model(args, **kwargs).to(device)\n",
    "    history = network.get_receptive_field() if args.type == 'tcn' else args.window_size // 2\n",
    "    criterion = get_loss_function(history, args, **kwargs)\n",
    "\n",
    "    optimizer = torch.optim.Adam(network.parameters(), args.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.75, verbose=True, eps=1e-12)\n",
    "    quiet_mode = False\n",
    "    use_scheduler = kwargs.get('use_scheduler', False)\n",
    "\n",
    "    log_file = None\n",
    "    if args.out_dir:\n",
    "        log_file = osp.join(args.out_dir, 'logs', 'log.txt')\n",
    "        if osp.exists(log_file):\n",
    "            if args.continue_from is None:\n",
    "                os.remove(log_file)\n",
    "            else:\n",
    "                copyfile(log_file, osp.join(args.out_dir, 'logs', 'log_old.txt'))\n",
    "\n",
    "    start_epoch = 0\n",
    "    train_losses_all, val_losses_all = [], []\n",
    "#     if args.continue_from is not None and osp.exists(args.continue_from):\n",
    "#         with open(osp.join(str(Path(args.continue_from).parents[1]), 'config.json'), 'r') as f:\n",
    "#             model_data = json.load(f)\n",
    "\n",
    "#         if device.type == 'cpu':\n",
    "#             checkpoints = torch.load(args.continue_from, map_location=lambda storage, location: storage)\n",
    "#         else:\n",
    "#             checkpoints = torch.load(args.continue_from, map_location={model_data['device']: args.device})\n",
    "\n",
    "#         start_epoch = checkpoints.get('epoch', 0)\n",
    "#         network.load_state_dict(checkpoints.get('model_state_dict'))\n",
    "#         optimizer.load_state_dict(checkpoints.get('optimizer_state_dict'))\n",
    "    if kwargs.get('force_lr', False):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = args.lr\n",
    "\n",
    "    step = 0\n",
    "    best_val_loss = np.inf\n",
    "    train_errs = np.zeros(args.epochs)\n",
    "\n",
    "    print(\"Starting from epoch {}\".format(start_epoch))\n",
    "    try:\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "            log_line = ''\n",
    "            network.train()\n",
    "            train_vel = MSEAverageMeter(3, [2], _output_channel)\n",
    "            train_loss = 0\n",
    "            start_t = time.time()\n",
    "\n",
    "            for bid, batch in enumerate(train_loader):\n",
    "                feat, targ, _, _ = batch\n",
    "                feat, targ = feat.to(device), targ.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                predicted = network(feat)\n",
    "                train_vel.add(predicted.cpu().detach().numpy(), targ.cpu().detach().numpy())\n",
    "                loss = criterion(predicted, targ)\n",
    "                train_loss += loss.cpu().detach().numpy()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            train_errs[epoch] = train_loss / train_mini_batches\n",
    "            train_losses_all.append(train_loss / train_mini_batches)\n",
    "            end_t = time.time()\n",
    "            if not quiet_mode:\n",
    "                print('-' * 25)\n",
    "                print('Epoch {}, time usage: {:.3f}s, loss: {}, vel_loss {}/{:.6f}'.format(\n",
    "                    epoch, end_t - start_t, train_errs[epoch], train_vel.get_channel_avg(), train_vel.get_total_avg()))\n",
    "            log_line = format_string(log_line, epoch, optimizer.param_groups[0]['lr'], train_errs[epoch],\n",
    "                                     *train_vel.get_channel_avg())\n",
    "\n",
    "            saved_model = False\n",
    "            if val_loader:\n",
    "                network.eval()\n",
    "                val_vel = MSEAverageMeter(3, [2], _output_channel)\n",
    "                val_loss = 0\n",
    "                for bid, batch in enumerate(val_loader):\n",
    "                    feat, targ, _, _ = batch\n",
    "                    feat, targ = feat.to(device), targ.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = network(feat)\n",
    "                    val_vel.add(pred.cpu().detach().numpy(), targ.cpu().detach().numpy())\n",
    "                    val_loss += criterion(pred, targ).cpu().detach().numpy()\n",
    "                val_loss = val_loss / val_mini_batches\n",
    "                val_losses_all.append(val_loss / val_mini_batches)\n",
    "                log_line = format_string(log_line, val_loss, *val_vel.get_channel_avg())\n",
    "                if not quiet_mode:\n",
    "                    print('Validation loss: {} vel_loss: {}/{:.6f}'.format(val_loss, val_vel.get_channel_avg(),\n",
    "                                                                           val_vel.get_total_avg()))\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    saved_model = True\n",
    "                    if args.out_dir:\n",
    "                        model_path = osp.join(args.out_dir, 'checkpoints', 'checkpoint_%d.pt' % epoch)\n",
    "                        torch.save({'model_state_dict': network.state_dict(),\n",
    "                                    'epoch': epoch,\n",
    "                                    'loss': train_errs[epoch],\n",
    "                                    'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "                        print('Best Validation Model saved to ', model_path)\n",
    "                if use_scheduler:\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "            if args.out_dir and not saved_model and (epoch + 1) % args.save_interval == 0:  # save even with validation\n",
    "                model_path = osp.join(args.out_dir, 'checkpoints', 'icheckpoint_%d.pt' % epoch)\n",
    "                torch.save({'model_state_dict': network.state_dict(),\n",
    "                            'epoch': epoch,\n",
    "                            'loss': train_errs[epoch],\n",
    "                            'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "                print('Model saved to ', model_path)\n",
    "\n",
    "            if log_file:\n",
    "                log_line += '\\n'\n",
    "                with open(log_file, 'a') as f:\n",
    "                    f.write(log_line)\n",
    "            if np.isnan(train_loss):\n",
    "                print(\"Invalid value. Stopping training.\")\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 60)\n",
    "        print('Early terminate')\n",
    "\n",
    "    print('Training completed')\n",
    "    if args.out_dir:\n",
    "        model_path = osp.join(args.out_dir, 'checkpoints', 'checkpoint_latest.pt')\n",
    "        torch.save({'model_state_dict': network.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "    return train_losses_all, val_losses_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a78ad39e-95fd-4a7a-b3f3-eaaa1a7c1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_traj_with_preds_global(dataset, preds, ind=None, seq_id=0, type='preds', **kwargs):\n",
    "    ind = ind if ind is not None else np.array([i[1] for i in dataset.index_map if i[0] == seq_id], dtype=np.int)\n",
    "\n",
    "    if type == 'gt':\n",
    "        pos = dataset.gt_pos[seq_id][:, :2]\n",
    "    else:\n",
    "        ts = dataset.ts[seq_id]\n",
    "        # Compute the global velocity from local velocity.\n",
    "        dts = np.mean(ts[ind[1:]] - ts[ind[:-1]])\n",
    "        pos = preds * dts\n",
    "        pos[0, :] = dataset.gt_pos[seq_id][0, :2]\n",
    "        pos = np.cumsum(pos, axis=0)\n",
    "    veloc = preds\n",
    "    ori = dataset.orientations[seq_id]\n",
    "\n",
    "    return pos, veloc, ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6adcac72-64a0-4bda-9e3b-0a67727b1baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def showAnimation(video_path):\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    video_widget = widgets.Video.from_file(video_path)\n",
    "    video_widget.controls = True\n",
    "    video_widget.autoplay = True\n",
    "    display(video_widget)\n",
    "\n",
    "def plotTrajectory(true_x_values, true_y_values, pred_x_values, pred_y_values):    \n",
    "    plt.figure('{}'.format(\"Trajectory\"), figsize=(16, 9))\n",
    "\n",
    "    length = len(true_x_values)\n",
    "    multiplier = 50\n",
    "    # creating a blank window\n",
    "    # for the animation\n",
    "    fig = plt.figure()\n",
    "    min_x_value = np.min([np.min(true_x_values), np.min(pred_x_values)])\n",
    "    max_x_value = np.max([np.max(true_x_values), np.max(pred_x_values)])\n",
    "    min_y_value = np.min([np.min(true_y_values), np.min(pred_y_values)])\n",
    "    max_y_value = np.max([np.max(true_y_values), np.max(pred_y_values)])\n",
    "    \n",
    "    \n",
    "    axis = plt.axes(xlim =(min_x_value, max_x_value), ylim =(min_y_value, max_y_value))\n",
    "    \n",
    "    line1, = axis.plot([], [], 'b', label='Line 1')\n",
    "    line2, = axis.plot([], [], 'r', label='Line 2')\n",
    "\n",
    "    def init():\n",
    "        line1.set_data([], [])\n",
    "        return line1,\n",
    "    def init():\n",
    "        line2.set_data([], [])\n",
    "        return line2,\n",
    "\n",
    "    # initializing empty values\n",
    "    # for x and y co-ordinates\n",
    "    true_xdata, true_ydata, pred_xdata, pred_ydata = [], [], [], []\n",
    "\n",
    "    # animation function\n",
    "    def animate(i):\n",
    "        true_x = true_x_values[i * multiplier]\n",
    "        true_y = true_y_values[i * multiplier]\n",
    "        pred_x = pred_x_values[i * multiplier]\n",
    "        pred_y = pred_y_values[i * multiplier]\n",
    "        \n",
    "        true_xdata.append(true_x)\n",
    "        true_ydata.append(true_y)\n",
    "        pred_xdata.append(pred_x)\n",
    "        pred_ydata.append(pred_y)\n",
    "        \n",
    "        line1.set_data(true_xdata, true_ydata)\n",
    "        line2.set_data(pred_xdata, pred_ydata)\n",
    "\n",
    "        return line1, line2\n",
    "\n",
    "    # calling the animation function\t\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func = init, frames = int(length/multiplier), interval = 0.01, blit = True)\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f3d029-1a20-49b4-888b-e5f69370e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, **kwargs):\n",
    "    global device, _output_channel\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if args.test_path is not None:\n",
    "        if args.test_path[-1] == '/':\n",
    "            args.test_path = args.test_path[:-1]\n",
    "        root_dir = osp.split(args.test_path)[0]\n",
    "        test_data_list = [osp.split(args.test_path)[1]]\n",
    "    elif args.test_list is not None:\n",
    "        root_dir = args.root_dir if args.root_dir else osp.split(args.test_list)[0]\n",
    "        with open(args.test_list) as f:\n",
    "            test_data_list = [s.strip().split(',')[0] for s in f.readlines() if len(s) > 0 and s[0] != '#']\n",
    "    else:\n",
    "        raise ValueError('Either test_path or test_list must be specified.')\n",
    "\n",
    "    # Load the first sequence to update the input and output size\n",
    "    _ = get_dataset(root_dir, [test_data_list[0]], args, mode='test')\n",
    "\n",
    "    if args.out_dir and not osp.exists(args.out_dir):\n",
    "        os.makedirs(args.out_dir)\n",
    "        \n",
    "    if not osp.isdir(osp.join(args.out_dir, 'Videos')) or not osp.isdir(osp.join(args.out_dir, 'Graphs')):\n",
    "        os.makedirs(osp.join(args.out_dir, 'Videos'))\n",
    "        os.makedirs(osp.join(args.out_dir, 'Graphs'))\n",
    "    \n",
    "    checkpoint = torch.load(args.model_path)\n",
    "    network = get_model(args, **kwargs)\n",
    "    network.load_state_dict(checkpoint.get('model_state_dict'))\n",
    "    network.eval().to(device)\n",
    "    print('Model {} loaded to device {}.'.format(args.model_path, device))\n",
    "\n",
    "    # log_file = None\n",
    "    # if args.test_list and args.out_dir:\n",
    "    #     log_file = osp.join(args.out_dir, osp.split(args.test_list)[-1].split('.')[0] + '_log.txt')\n",
    "    #     with open(log_file, 'w') as f:\n",
    "    #         f.write(args.model_path + '\\n')\n",
    "    #         f.write('Seq traj_len velocity ate rte\\n')\n",
    "\n",
    "    losses_vel = MSEAverageMeter(2, [1], _output_channel)\n",
    "    ate_all, rte_all = [], []\n",
    "    pred_per_min = 200 * 60\n",
    "\n",
    "    seq_dataset = get_dataset(root_dir, test_data_list, args, mode='test', **kwargs)\n",
    "\n",
    "    for idx, data in enumerate(test_data_list):\n",
    "        #assert data == osp.split(seq_dataset.data_path[idx])[1]\n",
    "        print(network)\n",
    "        feat, vel = seq_dataset.get_test_seq(idx)\n",
    "        feat = torch.Tensor(feat).to(device)\n",
    "        preds = np.squeeze(network(feat).cpu().detach().numpy())[-vel.shape[0]:, :_output_channel]\n",
    "        \n",
    "        ind = np.arange(vel.shape[0])\n",
    "        vel_losses = np.mean((vel - preds) ** 2, axis=0)\n",
    "        losses_vel.add(vel, preds)\n",
    "\n",
    "        print('Reconstructing trajectory')\n",
    "        pos_pred, gv_pred, _ = recon_traj_with_preds_global(seq_dataset, preds, ind=ind, type='pred', seq_id=idx)\n",
    "        pos_gt, gv_gt, _ = recon_traj_with_preds_global(seq_dataset, vel, ind=ind, type='gt', seq_id=idx)\n",
    "\n",
    "        \n",
    "\n",
    "        ate = compute_absolute_trajectory_error(pos_pred, pos_gt)\n",
    "        if pos_pred.shape[0] < pred_per_min:\n",
    "            ratio = pred_per_min / pos_pred.shape[0]\n",
    "            rte = compute_relative_trajectory_error(pos_pred, pos_gt, delta=pos_pred.shape[0] - 1) * ratio\n",
    "        else:\n",
    "            rte = compute_relative_trajectory_error(pos_pred, pos_gt, delta=pred_per_min)\n",
    "        pos_cum_error = np.linalg.norm(pos_pred - pos_gt, axis=1)\n",
    "        ate_all.append(ate)\n",
    "        rte_all.append(rte)\n",
    "\n",
    "        print('Sequence {}, Velocity loss {} / {}, ATE: {}, RTE:{}'.format(data, vel_losses, np.mean(vel_losses), ate,\n",
    "                                                                           rte))\n",
    "        log_line = format_string(data, np.mean(vel_losses), ate, rte)\n",
    "\n",
    "        kp = 2\n",
    "        targ_names = ['vx', 'vy']\n",
    "\n",
    "        if args.out_dir is not None and osp.isdir(args.out_dir) and args.saveAnim:\n",
    "            video_path = osp.join(args.out_dir, 'Videos/' + ''.join(data.split('/')) +'_Trajectory.mp4')\n",
    "            animation = plotTrajectory(pos_gt[:, 0], pos_gt[:, 1], pos_pred[:, 0], pos_pred[:, 1])\n",
    "            animation.save(video_path, writer = 'ffmpeg', fps = 50, bitrate = 700)\n",
    "            showAnimation(video_path)\n",
    "            \n",
    "        plt.figure('{}'.format(data), figsize=(16, 9))\n",
    "        plt.subplot2grid((kp, 2), (0, 0), rowspan=kp - 1)\n",
    "        plt.plot(pos_pred[:, 0], pos_pred[:, 1])\n",
    "        plt.plot(pos_gt[:, 0], pos_gt[:, 1])\n",
    "        plt.title(data)\n",
    "        plt.axis('equal')\n",
    "        plt.legend(['Predicted', 'Ground truth'])\n",
    "        plt.subplot2grid((kp, 2), (kp - 1, 0))\n",
    "        plt.plot(pos_cum_error)\n",
    "        plt.legend(['ATE:{:.3f}, RTE:{:.3f}'.format(ate_all[-1], rte_all[-1])])\n",
    "        for i in range(kp):\n",
    "            plt.subplot2grid((kp, 2), (i, 1))\n",
    "            plt.plot(ind, preds[:, i])\n",
    "            plt.plot(ind, vel[:, i])\n",
    "            plt.legend(['Predicted', 'Ground truth'])\n",
    "            plt.title('{}, error: {:.6f}'.format(targ_names[i], vel_losses[i]))\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if args.out_dir is not None and osp.isdir(args.out_dir):\n",
    "            np.save(osp.join(args.out_dir, 'Graphs/'+''.join(data.split('/')) + '_gsn.npy'),\n",
    "                    np.concatenate([pos_pred[:, :2], pos_gt[:, :2]], axis=1))\n",
    "            plt.savefig(osp.join(args.out_dir, 'Graphs/'+''.join(data.split('/')) + '_gsn.png'))\n",
    "            \n",
    "        if args.show_plot:\n",
    "            plt.show()\n",
    "\n",
    "        # if args.out_dir is not None and osp.isdir(args.out_dir):\n",
    "        #     np.save(osp.join(args.out_dir, '{}_{}.npy'.format(data, args.type)),\n",
    "        #             np.concatenate([pos_pred, pos_gt], axis=1))\n",
    "        # if args.out_dir is not None and osp.isdir(args.out_dir):\n",
    "        #     plt.savefig(osp.join(args.out_dir, '{}_{}.png'.format(data, args.type)))\n",
    "        plt.close('all')\n",
    "\n",
    "    ate_all = np.array(ate_all)\n",
    "    rte_all = np.array(rte_all)\n",
    "\n",
    "    measure = format_string('ATE', 'RTE', sep='\\t')\n",
    "    values = format_string(np.mean(ate_all), np.mean(rte_all), sep='\\t')\n",
    "    print(measure, '\\n', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b688bdad-7926-4966-aa05-8f2f53c95acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters into dataset initialisation \n",
      " feat_sigma: 2, targ_sigma: 2, random_shift: 5, transform: <transformations.ComposeTransform object at 0x7f9efc558f70>\n",
      "Parameters in dataset \n",
      "feat_sigma: 2, targ_sigma: 2, random_shift: 5, transform: None\n",
      "step_size: 10, window_size: 400\n",
      "Training set loaded. Time usage: 0.603s\n",
      "Parameters into dataset initialisation \n",
      " feat_sigma: 1, targ_sigma: -1, random_shift: 0, transform: <transformations.ComposeTransform object at 0x7f9efc558e20>\n",
      "Parameters in dataset \n",
      "feat_sigma: 1, targ_sigma: -1, random_shift: 0, transform: None\n",
      "step_size: 10, window_size: 400\n",
      "Validation set loaded\n",
      "\n",
      "Number of train samples: 32060\n",
      "Number of val samples: 12523\n",
      "Simple LSTM Network\n",
      "Network constructed. trainable parameters: 205832\n",
      "Starting from epoch 0\n",
      "-------------------------\n",
      "Epoch 0, time usage: 30.710s, loss: 14057.440046875, vel_loss [0.246843 0.452201]/0.349522\n",
      "Validation loss: 10095.985555013021 vel_loss: [0.208328 0.371472]/0.289900\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_0.pt\n",
      "-------------------------\n",
      "Epoch 1, time usage: 29.590s, loss: 8254.29949609375, vel_loss [0.281586 0.475731]/0.378659\n",
      "Validation loss: 4095.1834615071616 vel_loss: [0.316596 0.530305]/0.423451\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_1.pt\n",
      "-------------------------\n",
      "Epoch 2, time usage: 31.389s, loss: 3458.9188916015623, vel_loss [0.385107 0.699363]/0.542235\n",
      "Validation loss: 1794.047721862793 vel_loss: [0.380451 0.784225]/0.582338\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_2.pt\n",
      "-------------------------\n",
      "Epoch 3, time usage: 31.795s, loss: 1579.9441064453124, vel_loss [0.401589 0.790982]/0.596285\n",
      "Validation loss: 1046.6831525166829 vel_loss: [0.357805 0.753560]/0.555682\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_3.pt\n",
      "-------------------------\n",
      "Epoch 4, time usage: 30.398s, loss: 973.3188955078125, vel_loss [0.394040 0.661207]/0.527623\n",
      "Validation loss: 769.3822212219238 vel_loss: [0.359973 0.605799]/0.482886\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_4.pt\n",
      "-------------------------\n",
      "Epoch 5, time usage: 31.700s, loss: 706.32805859375, vel_loss [0.376488 0.537889]/0.457188\n",
      "Validation loss: 610.9121678670248 vel_loss: [0.343715 0.537766]/0.440741\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_5.pt\n",
      "-------------------------\n",
      "Epoch 6, time usage: 35.739s, loss: 548.3595349121093, vel_loss [0.366921 0.482027]/0.424474\n",
      "Validation loss: 485.45725949605304 vel_loss: [0.340361 0.472426]/0.406393\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_6.pt\n",
      "-------------------------\n",
      "Epoch 7, time usage: 32.404s, loss: 435.04143676757815, vel_loss [0.375549 0.430736]/0.403142\n",
      "Validation loss: 406.8623517354329 vel_loss: [0.353427 0.426044]/0.389735\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_7.pt\n",
      "-------------------------\n",
      "Epoch 8, time usage: 31.889s, loss: 358.9404626464844, vel_loss [0.381773 0.388873]/0.385323\n",
      "Validation loss: 388.3385473887126 vel_loss: [0.357308 0.378626]/0.367967\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_8.pt\n",
      "-------------------------\n",
      "Epoch 9, time usage: 34.794s, loss: 317.97874633789064, vel_loss [0.376377 0.358808]/0.367593\n",
      "Validation loss: 346.06474177042645 vel_loss: [0.339227 0.364771]/0.351999\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_9.pt\n",
      "-------------------------\n",
      "Epoch 10, time usage: 32.294s, loss: 293.3730828857422, vel_loss [0.367593 0.341297]/0.354445\n",
      "Validation loss: 334.29542032877606 vel_loss: [0.332520 0.344854]/0.338687\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_10.pt\n",
      "-------------------------\n",
      "Epoch 11, time usage: 32.595s, loss: 266.4552332763672, vel_loss [0.356303 0.322223]/0.339263\n",
      "Validation loss: 343.13661003112793 vel_loss: [0.328811 0.330773]/0.329792\n",
      "-------------------------\n",
      "Epoch 12, time usage: 31.899s, loss: 247.926048828125, vel_loss [0.349718 0.306924]/0.328321\n",
      "Validation loss: 287.11945152282715 vel_loss: [0.323248 0.316398]/0.319823\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_12.pt\n",
      "-------------------------\n",
      "Epoch 13, time usage: 33.205s, loss: 229.7019813232422, vel_loss [0.340570 0.294044]/0.317307\n",
      "Validation loss: 262.94640254974365 vel_loss: [0.311232 0.300546]/0.305889\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_13.pt\n",
      "-------------------------\n",
      "Epoch 14, time usage: 31.494s, loss: 217.57241027832032, vel_loss [0.331142 0.284281]/0.307711\n",
      "Validation loss: 230.62682723999023 vel_loss: [0.300307 0.280441]/0.290374\n",
      "Best Validation Model saved to  /home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset/pocket/lstm_outputs/checkpoints/checkpoint_14.pt\n",
      "-------------------------\n",
      "Epoch 15, time usage: 31.693s, loss: 201.37055834960938, vel_loss [0.321862 0.272133]/0.296997\n",
      "------------------------------------------------------------\n",
      "Early terminate\n",
      "Training completed\n",
      "Parameters into dataset initialisation \n",
      " feat_sigma: 2, targ_sigma: 2, random_shift: 5, transform: <transformations.ComposeTransform object at 0x7f9ef53472e0>\n",
      "Parameters in dataset \n",
      "feat_sigma: 2, targ_sigma: 2, random_shift: 5, transform: None\n",
      "step_size: 10, window_size: 400\n",
      "Training set loaded. Time usage: 0.602s\n",
      "Parameters into dataset initialisation \n",
      " feat_sigma: 1, targ_sigma: -1, random_shift: 0, transform: <transformations.ComposeTransform object at 0x7f9ef533a2b0>\n",
      "Parameters in dataset \n",
      "feat_sigma: 1, targ_sigma: -1, random_shift: 0, transform: None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m args\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mout_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/checkpoint_latest.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m args\u001b[38;5;241m.\u001b[39mout_dir \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mroot_dir, args\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(data_path_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m2\u001b[39m , i)\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m val_dataset, val_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mval_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation set loaded\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mget_dataset_from_list\u001b[0;34m(root_dir, list_path, args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(list_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(root_dir, data_list, args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m seq_type \u001b[38;5;241m=\u001b[39m OxfordGlobSpeedSequence\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameters into dataset initialisation \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m feat_sigma: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, targ_sigma: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, random_shift: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, transform: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mfeature_sigma, args\u001b[38;5;241m.\u001b[39mtarget_sigma, random_shift, transforms))\n\u001b[0;32m---> 26\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSequenceToSequenceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mgrv_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrv_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_shift\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_shift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_sigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_sigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_sigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mstep_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, window_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/imu_localisation/Oxiod/source/data_Oxiod.py:230\u001b[0m, in \u001b[0;36mSequenceToSequenceDataset.__init__\u001b[0;34m(self, seq_type, root_dir, data_list, cache_path, step_size, window_size, random_shift, transform, feature_sigma, target_sigma, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(window_size \u001b[38;5;241m+\u001b[39m random_shift, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], step_size):\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bad_data[j \u001b[38;5;241m-\u001b[39m window_size \u001b[38;5;241m-\u001b[39m random_shift:j \u001b[38;5;241m+\u001b[39m random_shift]\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 230\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_map\u001b[38;5;241m.\u001b[39mappend([i, j])\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_map)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data_path_list = ['handbag', 'handheld', 'pocket', 'running', 'slow walking', 'trolley', 'large scale']\n",
    "data_path_list = ['pocket', 'running', 'slow walking', 'trolley', 'large scale']\n",
    "i = 1\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "fig.tight_layout()\n",
    "for data in data_path_list:\n",
    "    args.root_dir = osp.join('/home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset', data)\n",
    "    args.train_list = osp.join(args.root_dir, 'Train.txt')\n",
    "    args.val_list = osp.join(args.root_dir, 'Validation.txt')\n",
    "    args.model_path = osp.join(args.out_dir, 'checkpoints/checkpoint_latest.pt')\n",
    "    args.out_dir = osp.join(args.root_dir, args.type + '_outputs')\n",
    "    train_losses, val_losses = train(args)\n",
    "    plt.subplot(math.ceil(len(data_path_list) / 2), 2 , i)\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.title(data+' training loss')\n",
    "    plt.legend(['train_loss', 'val_loss'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef679dc-a9a6-4d07-923b-9d17c5452505",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.saveAnim = False\n",
    "args.batch_size = 1\n",
    "args.target_sigma = -1\n",
    "data_path_list = ['handbag', 'handheld', 'pocket', 'running', 'slow walking', 'trolley', 'large scale']\n",
    "for data in data_path_list:\n",
    "    print(f'=============================================================================outputs for {data} ==============================================================')\n",
    "    args.root_dir = osp.join('/home/jovyan/localisation_datasets/Oxford Inertial Odometry Dataset', data)\n",
    "    args.test_list = osp.join(args.root_dir, 'Test.txt')\n",
    "    args.out_dir = osp.join(args.root_dir, args.type + '_outputs')\n",
    "    args.model_path = osp.join(args.out_dir, 'checkpoints/checkpoint_latest.pt')\n",
    "    test(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
